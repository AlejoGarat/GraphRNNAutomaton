{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNN)\n",
    "Source: https://www.kaggle.com/code/fareselmenshawii/rnn-from-scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy as np\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator:\n",
    "    \"\"\"\n",
    "    A class for generating input and output examples for a character-level language model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        \"\"\"\n",
    "        Initializes a DataGenerator object.\n",
    "\n",
    "        Args:\n",
    "            path (str): The path to the text file containing the training data.\n",
    "        \"\"\"\n",
    "        self.path = path\n",
    "        \n",
    "        # Read in data from file and convert to lowercase\n",
    "        with open(path) as f:\n",
    "            data = f.read().lower()\n",
    "        \n",
    "        # Create list of unique characters in the data\n",
    "        self.chars = list(set(data))\n",
    "        \n",
    "        # Create dictionaries mapping characters to and from their index in the list of unique characters\n",
    "        self.char_to_idx = {ch: i for (i, ch) in enumerate(self.chars)}\n",
    "        self.idx_to_char = {i: ch for (i, ch) in enumerate(self.chars)}\n",
    "        \n",
    "        # Set the size of the vocabulary (i.e. number of unique characters)\n",
    "        self.vocab_size = len(self.chars)\n",
    "        \n",
    "        # Read in examples from file and convert to lowercase, removing leading/trailing white space\n",
    "        with open(path) as f:\n",
    "            examples = f.readlines()\n",
    "        self.examples = [x.lower().strip() for x in examples]\n",
    " \n",
    "    def generate_example(self, idx):\n",
    "        \"\"\"\n",
    "        Generates an input/output example for the language model based on the given index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the example to generate.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing the input and output arrays for the example.\n",
    "        \"\"\"\n",
    "        example_chars = self.examples[idx]\n",
    "        \n",
    "        # Convert the characters in the example to their corresponding indices in the list of unique characters\n",
    "        example_char_idx = [self.char_to_idx[char] for char in example_chars]\n",
    "        \n",
    "        # Add newline character as the first character in the input array, and as the last character in the output array\n",
    "        X = [self.char_to_idx['\\n']] + example_char_idx\n",
    "        Y = example_char_idx + [self.char_to_idx['\\n']]\n",
    "        \n",
    "        return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    \"\"\"\n",
    "    A class used to represent a Recurrent Neural Network (RNN).\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    hidden_size : int\n",
    "        The number of hidden units in the RNN.\n",
    "    vocab_size : int\n",
    "        The size of the vocabulary used by the RNN.\n",
    "    sequence_length : int\n",
    "        The length of the input sequences fed to the RNN.\n",
    "    learning_rate : float\n",
    "        The learning rate used during training.\n",
    "    is_initialized : bool\n",
    "        Indicates whether the AdamW parameters has been initialized.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    __init__(hidden_size, vocab_size, sequence_length, learning_rate)\n",
    "        Initializes an instance of the RNN class.\n",
    "    \n",
    "    forward(self, X, a_prev)\n",
    "     Computes the forward pass of the RNN.\n",
    "     \n",
    "    softmax(self, x)\n",
    "       Computes the softmax activation function for a given input array. \n",
    "       \n",
    "    backward(self,x, a, y_preds, targets)    \n",
    "        Implements the backward pass of the RNN.\n",
    "        \n",
    "   loss(self, y_preds, targets)\n",
    "     Computes the cross-entropy loss for a given sequence of predicted probabilities and true targets. \n",
    "     \n",
    "    adamw(self, beta1=0.9, beta2=0.999, epsilon=1e-8, L2_reg=1e-4)\n",
    "       Updates the RNN's parameters using the AdamW optimization algorithm.\n",
    "       \n",
    "    train(self, generated_names=5)\n",
    "       Trains the RNN on a dataset using backpropagation through time (BPTT).   \n",
    "       \n",
    "   predict(self, start)\n",
    "        Generates a sequence of characters using the trained self, starting from the given start sequence.\n",
    "        The generated sequence may contain a maximum of 50 characters or a newline character.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size, data_generator, sequence_length, learning_rate):\n",
    "        \"\"\"\n",
    "        Initializes an instance of the RNN class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        hidden_size : int\n",
    "            The number of hidden units in the RNN.\n",
    "        vocab_size : int\n",
    "            The size of the vocabulary used by the RNN.\n",
    "        sequence_length : int\n",
    "            The length of the input sequences fed to the RNN.\n",
    "        learning_rate : float\n",
    "            The learning rate used during training.\n",
    "        \"\"\"\n",
    "\n",
    "        # hyper parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.data_generator = data_generator\n",
    "        self.vocab_size = self.data_generator.vocab_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.learning_rate = learning_rate\n",
    "        self.X = None\n",
    "\n",
    "        # model parameters\n",
    "        self.Wax = np.random.uniform(-np.sqrt(1. / self.vocab_size), np.sqrt(1. / self.vocab_size), (hidden_size, self.vocab_size))\n",
    "        self.Waa = np.random.uniform(-np.sqrt(1. / hidden_size), np.sqrt(1. / hidden_size), (hidden_size, hidden_size))\n",
    "        self.Wya = np.random.uniform(-np.sqrt(1. / hidden_size), np.sqrt(1. / hidden_size), (self.vocab_size, hidden_size))\n",
    "        self.ba = np.zeros((hidden_size, 1))  \n",
    "        self.by = np.zeros((self.vocab_size, 1))\n",
    "        \n",
    "        # Initialize gradients\n",
    "        self.dWax, self.dWaa, self.dWya = np.zeros_like(self.Wax), np.zeros_like(self.Waa), np.zeros_like(self.Wya)\n",
    "        self.dba, self.dby = np.zeros_like(self.ba), np.zeros_like(self.by)\n",
    "        \n",
    "        # parameter update with AdamW\n",
    "        self.mWax = np.zeros_like(self.Wax)\n",
    "        self.vWax = np.zeros_like(self.Wax)\n",
    "        self.mWaa = np.zeros_like(self.Waa)\n",
    "        self.vWaa = np.zeros_like(self.Waa)\n",
    "        self.mWya = np.zeros_like(self.Wya)\n",
    "        self.vWya = np.zeros_like(self.Wya)\n",
    "        self.mba = np.zeros_like(self.ba)\n",
    "        self.vba = np.zeros_like(self.ba)\n",
    "        self.mby = np.zeros_like(self.by)\n",
    "        self.vby = np.zeros_like(self.by)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        \"\"\"\n",
    "        Computes the softmax activation function for a given input array.\n",
    "\n",
    "        Parameters:\n",
    "            x (ndarray): Input array.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: Array of the same shape as `x`, containing the softmax activation values.\n",
    "        \"\"\"\n",
    "        # shift the input to prevent overflow when computing the exponentials\n",
    "        x = x - np.max(x)\n",
    "        # compute the exponentials of the shifted input\n",
    "        p = np.exp(x)\n",
    "        # normalize the exponentials by dividing by their sum\n",
    "        return p / np.sum(p)\n",
    "\n",
    "    def forward(self, X, a_prev):\n",
    "        \"\"\"\n",
    "        Compute the forward pass of the RNN.\n",
    "\n",
    "        Parameters:\n",
    "        X (ndarray): Input data of shape (seq_length, vocab_size)\n",
    "        a_prev (ndarray): Activation of the previous time step of shape (hidden_size, 1)\n",
    "\n",
    "        Returns:\n",
    "        x (dict): Dictionary of input data of shape (seq_length, vocab_size, 1), with keys from 0 to seq_length-1\n",
    "        a (dict): Dictionary of hidden activations for each time step, with keys from 0 to seq_length-1\n",
    "        y_pred (dict): Dictionary of output probabilities for each time step, with keys from 0 to seq_length-1\n",
    "        \"\"\"\n",
    "        # Initialize dictionaries to store activations and output probabilities.\n",
    "        x, a, y_pred = {}, {}, {}\n",
    "\n",
    "        # Store the input data in the class variable for later use in the backward pass.\n",
    "        self.X = X\n",
    "\n",
    "        # Set the initial activation to the previous activation.\n",
    "        a[-1] = np.copy(a_prev)\n",
    "        # iterate over each time step in the input sequence\n",
    "        for t in range(len(self.X)): \n",
    "            # get the input at the current time step\n",
    "            x[t] = np.zeros((self.vocab_size,1)) \n",
    "            if (self.X[t] != None):\n",
    "                x[t][self.X[t]] = 1\n",
    "            # compute the hidden activation at the current time step\n",
    "            a[t] = np.tanh(np.dot(self.Wax, x[t]) + np.dot(self.Waa, a[t - 1]) + self.ba)\n",
    "            # compute the output probabilities at the current time step\n",
    "            y_pred[t] = self.softmax(np.dot(self.Wya, a[t]) + self.by)\n",
    "            # add an extra dimension to X to make it compatible with the shape of the input to the backward pass\n",
    "         # return the input, hidden activations, and output probabilities at each time step\n",
    "        return x, a, y_pred \n",
    "    \n",
    "    def backward(self,x, a, y_preds, targets):\n",
    "        \"\"\"\n",
    "        Implement the backward pass of the RNN.\n",
    "\n",
    "        Args:\n",
    "        x -- (dict) of input characters (as one-hot encoding vectors) for each time-step, shape (vocab_size, sequence_length)\n",
    "        a -- (dict) of hidden state vectors for each time-step, shape (hidden_size, sequence_length)\n",
    "        y_preds -- (dict) of output probability vectors (after softmax) for each time-step, shape (vocab_size, sequence_length)\n",
    "        targets -- (list) of integer target characters (indices of characters in the vocabulary) for each time-step, shape (1, sequence_length)\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "\n",
    "        \"\"\"\n",
    "        # Initialize derivative of hidden state for the last time-step\n",
    "        da_next = np.zeros_like(a[0])\n",
    "\n",
    "        # Loop through the input sequence backwards\n",
    "        for t in reversed(range(len(self.X))):\n",
    "            # Calculate derivative of output probability vector\n",
    "            dy_preds = np.copy(y_preds[t])\n",
    "            dy_preds[targets[t]] -= 1\n",
    "\n",
    "            # Calculate derivative of hidden state\n",
    "            da = np.dot(self.Waa.T, da_next) + np.dot(self.Wya.T, dy_preds)\n",
    "            dtanh = (1 - np.power(a[t], 2))\n",
    "            da_unactivated = dtanh * da\n",
    "\n",
    "            # Calculate gradients\n",
    "            self.dba += da_unactivated\n",
    "            self.dWax += np.dot(da_unactivated, x[t].T)\n",
    "            self.dWaa += np.dot(da_unactivated, a[t - 1].T)\n",
    "\n",
    "            # Update derivative of hidden state for the next iteration\n",
    "            da_next = da_unactivated\n",
    "\n",
    "            # Calculate gradient for output weight matrix\n",
    "            self.dWya += np.dot(dy_preds, a[t].T)\n",
    "\n",
    "            # clip gradients to avoid exploding gradients\n",
    "            for grad in [self.dWax, self.dWaa, self.dWya, self.dba, self.dby]:\n",
    "                np.clip(grad, -1, 1, out=grad)\n",
    " \n",
    "    def loss(self, y_preds, targets):\n",
    "        \"\"\"\n",
    "        Computes the cross-entropy loss for a given sequence of predicted probabilities and true targets.\n",
    "\n",
    "        Parameters:\n",
    "            y_preds (ndarray): Array of shape (sequence_length, vocab_size) containing the predicted probabilities for each time step.\n",
    "            targets (ndarray): Array of shape (sequence_length, 1) containing the true targets for each time step.\n",
    "\n",
    "        Returns:\n",
    "            float: Cross-entropy loss.\n",
    "        \"\"\"\n",
    "        # calculate cross-entropy loss\n",
    "        return sum(-np.log(y_preds[t][targets[t], 0]) for t in range(len(self.X)))\n",
    "    \n",
    "    def adamw(self, beta1=0.9, beta2=0.999, epsilon=1e-8, L2_reg=1e-4):\n",
    "        \"\"\"\n",
    "        Updates the RNN's parameters using the AdamW optimization algorithm.\n",
    "        \"\"\"\n",
    "        # AdamW update for Wax\n",
    "        self.mWax = beta1 * self.mWax + (1 - beta1) * self.dWax\n",
    "        self.vWax = beta2 * self.vWax + (1 - beta2) * np.square(self.dWax)\n",
    "        m_hat = self.mWax / (1 - beta1)\n",
    "        v_hat = self.vWax / (1 - beta2)\n",
    "        self.Wax -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.Wax)\n",
    "\n",
    "        # AdamW update for Waa\n",
    "        self.mWaa = beta1 * self.mWaa + (1 - beta1) * self.dWaa\n",
    "        self.vWaa = beta2 * self.vWaa + (1 - beta2) * np.square(self.dWaa)\n",
    "        m_hat = self.mWaa / (1 - beta1)\n",
    "        v_hat = self.vWaa / (1 - beta2)\n",
    "        self.Waa -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.Waa)\n",
    "\n",
    "        # AdamW update for Wya\n",
    "        self.mWya = beta1 * self.mWya + (1 - beta1) * self.dWya\n",
    "        self.vWya = beta2 * self.vWya + (1 - beta2) * np.square(self.dWya)\n",
    "        m_hat = self.mWya / (1 - beta1)\n",
    "        v_hat = self.vWya / (1 - beta2)\n",
    "        self.Wya -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.Wya)\n",
    "\n",
    "        # AdamW update for ba\n",
    "        self.mba = beta1 * self.mba + (1 - beta1) * self.dba\n",
    "        self.vba = beta2 * self.vba + (1 - beta2) * np.square(self.dba)\n",
    "        m_hat = self.mba / (1 - beta1)\n",
    "        v_hat = self.vba / (1 - beta2)\n",
    "        self.ba -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.ba)\n",
    "\n",
    "        # AdamW update for by\n",
    "        self.mby = beta1 * self.mby + (1 - beta1) * self.dby\n",
    "        self.vby = beta2 * self.vby + (1 - beta2) * np.square(self.dby)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"\n",
    "        Sample a sequence of characters from the RNN.\n",
    "\n",
    "        Args:\n",
    "            None\n",
    "\n",
    "        Returns:\n",
    "            list: A list of integers representing the generated sequence.\n",
    "        \"\"\"\n",
    "        # initialize input and hidden state\n",
    "        x = np.zeros((self.vocab_size, 1))\n",
    "        a_prev = np.zeros((self.hidden_size, 1))\n",
    "\n",
    "        # create an empty list to store the generated character indices\n",
    "        indices = []\n",
    "\n",
    "        # idx is a flag to detect a newline character, initialize it to -1\n",
    "        idx = -1\n",
    "\n",
    "        # generate sequence of characters\n",
    "        counter = 0\n",
    "        max_chars = 50 # maximum number of characters to generate\n",
    "        newline_character = self.data_generator.char_to_idx['\\n'] # the newline character\n",
    "\n",
    "        while (idx != newline_character and counter != max_chars):\n",
    "            # compute the hidden state\n",
    "            a = np.tanh(np.dot(self.Wax, x) + np.dot(self.Waa, a_prev) + self.ba)\n",
    "\n",
    "            # compute the output probabilities\n",
    "            y = self.softmax(np.dot(self.Wya, a) + self.by)\n",
    "\n",
    "            # sample the next character from the output probabilities\n",
    "            idx = np.random.choice(list(range(self.vocab_size)), p=y.ravel())\n",
    "\n",
    "            # set the input for the next time step\n",
    "            x = np.zeros((self.vocab_size, 1))\n",
    "            x[idx] = 1\n",
    "\n",
    "            # store the sampled character index in the list\n",
    "            indices.append(idx)\n",
    "\n",
    "            # update the previous hidden state\n",
    "            a_prev = a\n",
    "\n",
    "            # increment the counter\n",
    "            counter += 1\n",
    "\n",
    "        # return the list of sampled character indices\n",
    "        return indices\n",
    "\n",
    "        \n",
    "    def train(self, generated_names=5):\n",
    "        \"\"\"\n",
    "        Train the RNN on a dataset using backpropagation through time (BPTT).\n",
    "\n",
    "        Args:\n",
    "        - generated_names: an integer indicating how many example names to generate during training.\n",
    "\n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "\n",
    "        iter_num = 0\n",
    "        threshold = 5 # stopping criterion for training\n",
    "        smooth_loss = -np.log(1.0 / self.data_generator.vocab_size) * self.sequence_length  # initialize loss\n",
    "\n",
    "        while (smooth_loss > threshold):\n",
    "            a_prev = np.zeros((self.hidden_size, 1))\n",
    "            idx = iter_num % self.vocab_size\n",
    "            # get a batch of inputs and targets\n",
    "            inputs, targets = self.data_generator.generate_example(idx)\n",
    "\n",
    "            # forward pass\n",
    "            x, a, y_pred  = self.forward(inputs, a_prev)\n",
    "\n",
    "            # backward pass\n",
    "            self.backward(x, a, y_pred, targets)\n",
    "\n",
    "            # calculate and update loss\n",
    "            loss = self.loss(y_pred, targets)\n",
    "            self.adamw()\n",
    "            smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "            # update previous hidden state for the next batch\n",
    "            a_prev = a[len(self.X) - 1]\n",
    "            # print progress every 500 iterations\n",
    "            if iter_num % 500 == 0:\n",
    "                print(\"\\n\\niter :%d, loss:%f\\n\" % (iter_num, smooth_loss))\n",
    "                for i in range(generated_names):\n",
    "                    sample_idx = self.sample()\n",
    "                    txt = ''.join(self.data_generator.idx_to_char[idx] for idx in sample_idx)\n",
    "                    txt = txt.title()  # capitalize first character \n",
    "                    print ('%s' % (txt, ), end='')\n",
    "            iter_num += 1\n",
    "    \n",
    "    def predict(self, start):\n",
    "        \"\"\"\n",
    "        Generate a sequence of characters using the trained self, starting from the given start sequence.\n",
    "        The generated sequence may contain a maximum of 50 characters or a newline character.\n",
    "\n",
    "        Args:\n",
    "        - start: a string containing the start sequence\n",
    "\n",
    "        Returns:\n",
    "        - txt: a string containing the generated sequence\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize input vector and previous hidden state\n",
    "        x = np.zeros((self.vocab_size, 1))\n",
    "        a_prev = np.zeros((self.hidden_size, 1))\n",
    "\n",
    "        # Convert start sequence to indices\n",
    "        chars = [ch for ch in start]\n",
    "        idxes = []\n",
    "        for i in range(len(chars)):\n",
    "            idx = self.data_generator.char_to_idx[chars[i]]\n",
    "            x[idx] = 1\n",
    "            idxes.append(idx)\n",
    "\n",
    "        # Generate sequence\n",
    "        max_chars = 50  # maximum number of characters to generate\n",
    "        newline_character = self.data_generator.char_to_idx['\\n']  # the newline character\n",
    "        counter = 0\n",
    "        while (idx != newline_character and counter != max_chars):\n",
    "            # Compute next hidden state and predicted character\n",
    "            a = np.tanh(np.dot(self.Wax, x) + np.dot(self.Waa, a_prev) + self.ba)\n",
    "            y_pred = self.softmax(np.dot(self.Wya, a) + self.by)\n",
    "            idx = np.random.choice(range(self.vocab_size), p=y_pred.ravel())\n",
    "\n",
    "            # Update input vector, previous hidden state, and indices\n",
    "            x = np.zeros((self.vocab_size, 1))\n",
    "            x[idx] = 1\n",
    "            a_prev = a\n",
    "            idxes.append(idx)\n",
    "            counter += 1\n",
    "\n",
    "        # Convert indices to characters and concatenate into a string\n",
    "        txt = ''.join(self.data_generator.idx_to_char[i] for i in idxes)\n",
    "\n",
    "        # Remove newline character if it exists at the end of the generated sequence\n",
    "        if txt[-1] == '\\n':\n",
    "            txt = txt[:-1]\n",
    "\n",
    "        return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iter :0, loss:82.359270\n",
      "\n",
      "Obwouljjvahjvee\n",
      "Yep\n",
      "WxbnqhitsutypgzgyijwzkwvoejhweghglwzrlqfaarlduqnekIyoxrgx\n",
      "Ivhnmamusqqlhxgbtadaxvkmukbdf\n",
      "\n",
      "\n",
      "iter :500, loss:58.888074\n",
      "\n",
      "Bdayolsaur\n",
      "Veaalnamaorus\n",
      "Acpoopaurus\n",
      "Aapiotant\n",
      "Oaoleuput\n",
      "\n",
      "\n",
      "iter :1000, loss:41.241823\n",
      "\n",
      "Aegoehabpris\n",
      "Elysaorus\n",
      "Irdetosaurus\n",
      "Adasautus\n",
      "Avolisaatus\n",
      "\n",
      "\n",
      "iter :1500, loss:28.867886\n",
      "\n",
      "Actistorus\n",
      "Acoichopathur\n",
      "Aetosteon\n",
      "Usasaurus\n",
      "Adymthophilis\n",
      "\n",
      "\n",
      "iter :2000, loss:20.294475\n",
      "\n",
      "Vegypeosaurus\n",
      "Abrosauous\n",
      "Actoosaurus\n",
      "Abrosaurus\n",
      "\n",
      "\n",
      "\n",
      "iter :2500, loss:14.659064\n",
      "\n",
      "\n",
      "Toolosaurus\n",
      "Ljelosaurus\n",
      "Actonyxafromimus\n",
      "Acheroraptoous\n",
      "\n",
      "\n",
      "iter :3000, loss:10.937260\n",
      "\n",
      "Adedonyx\n",
      "Acrostavus\n",
      "Aorostavus\n",
      "Achillobator\n",
      "Ctelosaurus\n",
      "\n",
      "\n",
      "iter :3500, loss:8.557766\n",
      "\n",
      "Dailolophus\n",
      "Sausaurus\n",
      "Actonanapholis\n",
      "Acrocanthosaurus\n",
      "Caelolopausaurus\n",
      "\n",
      "\n",
      "iter :4000, loss:7.026997\n",
      "\n",
      "Dapyornithomimus\n",
      "Aadallahsaurus\n",
      "Lieloopptoros\n",
      "Aardonyx\n",
      "Adasaurus\n",
      "\n",
      "\n",
      "iter :4500, loss:6.047316\n",
      "\n",
      "Abrictosaurus\n",
      "Airdonyxbhaus\n",
      "Apteonaxbus\n",
      "Gyeopyxacrosaurus\n",
      "Adasaurus\n",
      "\n",
      "\n",
      "iter :5000, loss:5.414247\n",
      "\n",
      "Gyllosaurus\n",
      "Eocanthotholim\n",
      "Actiosaurus\n",
      "Erosaurus\n",
      "Adrothelisaurus\n",
      "\n",
      "\n",
      "iter :5500, loss:5.069565\n",
      "\n",
      "Apponyxathimus\n",
      "Aeposaurus\n",
      "Aptooanyx\n",
      "Acheloraptor\n",
      "Ovelaptos\n"
     ]
    }
   ],
   "source": [
    "data_generator = DataGenerator('dinos.txt')\n",
    "rnn = RNN(hidden_size=200,data_generator=data_generator, sequence_length=25, learning_rate=1e-3)\n",
    "rnn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tyraornsaur'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.predict(\"tyra\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
